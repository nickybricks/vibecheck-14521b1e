---
phase: 02-data-pipeline
plan: 04
type: execute
wave: 3
depends_on: ["02", "03"]
files_modified:
  - backend/main.py
  - backend/pipeline/scheduler.py
  - backend/api/routes/health.py
  - backend/db/models.py
autonomous: false

must_haves:
  truths:
    - "APScheduler starts with FastAPI lifespan and registers 2 jobs (news, stories)"
    - "News job runs every 15 minutes, stories job runs every 60 minutes"
    - "Scheduler logs each job execution with status, duration, and errors"
    - "Health endpoint reports when each job last ran and alerts if overdue"
    - "Scheduler continues running even if individual jobs fail"
  artifacts:
    - path: "backend/pipeline/scheduler.py"
      provides: "AsyncIOScheduler with job registration and health tracking"
      min_lines: 80
      contains: "AsyncIOScheduler.*add_job.*poll_news_job.*poll_stories_job"
    - path: "backend/main.py"
      provides: "FastAPI lifespan with scheduler startup/shutdown"
      contains: "@asynccontextmanager.*lifespan.*scheduler\\.start"
    - path: "backend/api/routes/health.py"
      provides: "Scheduler health check endpoint"
      contains: "GET.*health/scheduler.*job_last_run"
    - path: "backend/db/models.py"
      provides: "SchedulerExecutionLog model for audit trail"
      contains: "class SchedulerExecutionLog.*execution_id.*job_name.*status"
  key_links:
    - from: "backend/pipeline/scheduler.py"
      to: "backend/pipeline/jobs/news_job.py"
      via: "import poll_news_job"
      pattern: "from backend\\.pipeline\\.jobs\\.news_job import poll_news_job"
    - from: "backend/pipeline/scheduler.py"
      to: "backend/pipeline/jobs/stories_job.py"
      via: "import poll_stories_job"
      pattern: "from backend\\.pipeline\\.jobs\\.stories_job import poll_stories_job"
    - from: "backend/main.py"
      to: "backend/pipeline/scheduler.py"
      via: "import scheduler"
      pattern: "from backend\\.pipeline\\.scheduler import scheduler"
---

<objective>
Integrate APScheduler with FastAPI for automated news and story polling with health monitoring.

Purpose: Complete the data pipeline by scheduling periodic job execution (news every 15 min, stories every 60 min), implementing health checks to detect silent failures, and logging execution history for observability.
Output: Production-ready scheduled data pipeline with automated polling and monitoring.
</objective>

<execution_context>
@/Users/daniswhoiam/.claude/get-shit-done/workflows/execute-plan.md
@/Users/daniswhoiam/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-pipeline/02-RESEARCH.md

# From Plan 02
poll_news_job(db_session) in backend/pipeline/jobs/news_job.py

# From Plan 03
poll_stories_job(db_session) in backend/pipeline/jobs/stories_job.py

# From Phase 1
FastAPI app with lifespan in backend/main.py
Health endpoint in backend/api/routes/health.py (will extend)
AsyncSession in backend/db/session.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SchedulerExecutionLog model for audit trail</name>
  <files>backend/db/models.py</files>
  <action>
Add SchedulerExecutionLog model to backend/db/models.py for tracking job executions:

**Add to models.py:**

```python
class SchedulerExecutionLog(Base):
    __tablename__ = "scheduler_execution_log"

    id: Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    execution_id: Mapped[str] = mapped_column(String(36), nullable=False, index=True)  # UUID
    job_name: Mapped[str] = mapped_column(String(50), nullable=False, index=True)
    status: Mapped[str] = mapped_column(String(20), nullable=False)  # success, failure, partial_failure
    started_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=False)
    completed_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), nullable=True)
    duration_seconds: Mapped[float] = mapped_column(Float, nullable=True)
    error_message: Mapped[str | None] = mapped_column(Text, nullable=True)
    metadata: Mapped[dict | None] = mapped_column(JSON, nullable=True)  # Stores execution stats (articles_stored, entities_processed, etc.)

    created_at: Mapped[datetime] = mapped_column(DateTime(timezone=True), server_default=func.now())
```

**Indexes:**
- execution_id: Fast lookup for specific execution
- job_name: Filter by job type
- started_at: Time-based queries (last N executions)

**Migration:**
After modifying model, generate and apply migration:
```bash
alembic revision --autogenerate -m "add scheduler_execution_log table"
alembic upgrade head
```

DO NOT run migration in this task - just modify the model. Migration will be run during verification.
  </action>
  <verify>
1. Check model exists: `grep 'class SchedulerExecutionLog' backend/db/models.py`
2. Generate migration: `docker-compose exec backend alembic revision --autogenerate -m "add scheduler_execution_log table"`
3. Apply migration: `docker-compose exec backend alembic upgrade head`
4. Verify schema: `docker-compose exec postgres psql -U vibecheck -d vibecheck -c "\d scheduler_execution_log"`
  </verify>
  <done>
SchedulerExecutionLog model exists with execution tracking fields. Migration applied successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create APScheduler integration with job registration</name>
  <files>backend/pipeline/scheduler.py</files>
  <action>
Create backend/pipeline/scheduler.py for AsyncIOScheduler setup and job registration:

**Imports:**
- apscheduler.schedulers.asyncio: AsyncIOScheduler
- datetime, UTC
- structlog
- poll_news_job from backend.pipeline.jobs.news_job
- poll_stories_job from backend.pipeline.jobs.stories_job
- get_async_session from backend.db.session
- SchedulerExecutionLog from backend.db.models

**Module-level variables:**
```python
scheduler = AsyncIOScheduler(timezone='UTC')
job_last_run = {}  # Track last execution time per job {job_name: datetime}
```

**Implement:**

1. **async wrapped_job_execution(job_name: str, job_func, *args, **kwargs):**
   - Record start time and generate execution_id
   - Update job_last_run[job_name] = datetime.now(UTC)
   - Try:
     - Call await job_func(*args, **kwargs)
     - Capture result dict with execution stats
     - Log SchedulerExecutionLog with status="success", metadata=result
   - Except Exception as e:
     - Log SchedulerExecutionLog with status="failure", error_message=str(e)
     - Log error with structlog
     - Re-raise exception (APScheduler handles it gracefully)

2. **async poll_news_job_wrapper():**
   - Get async session from get_async_session()
   - Call wrapped_job_execution("poll_news", poll_news_job, db_session=session)
   - Close session after execution

3. **async poll_stories_job_wrapper():**
   - Get async session from get_async_session()
   - Call wrapped_job_execution("poll_stories", poll_stories_job, db_session=session)
   - Close session after execution

4. **def setup_jobs():**
   - Add news job:
     ```python
     scheduler.add_job(
         poll_news_job_wrapper,
         "interval",
         minutes=15,
         id="poll_news",
         name="Poll AskNews News Endpoint",
         max_instances=1,  # Only one instance runs at a time
         coalesce=True,    # Skip missed runs if system was down
         misfire_grace_time=60,  # Don't run if >1min late
     )
     ```
   - Add stories job:
     ```python
     scheduler.add_job(
         poll_stories_job_wrapper,
         "interval",
         hours=1,
         id="poll_stories",
         name="Poll AskNews Stories Endpoint",
         max_instances=1,
         coalesce=True,
         misfire_grace_time=60,
     )
     ```
   - Log "scheduler_jobs_registered" with job count

5. **async get_job_health() -> dict:**
   - Get current time
   - Define expected intervals: {"poll_news": 15*60, "poll_stories": 60*60}
   - For each job:
     - Check job_last_run
     - If never run, status="not_started"
     - If last_run age > 2x interval, status="unhealthy"
     - Else status="healthy"
   - Return dict with overall status and per-job details

**DO NOT:**
- Start scheduler here (main.py handles startup)
- Create database sessions manually (use get_async_session)
- Run jobs synchronously (all async)
  </action>
  <verify>
1. Check file exists: `ls backend/pipeline/scheduler.py`
2. Verify scheduler instance: `grep 'scheduler = AsyncIOScheduler' backend/pipeline/scheduler.py`
3. Verify job wrappers: `grep -E 'async def poll_news_job_wrapper|async def poll_stories_job_wrapper' backend/pipeline/scheduler.py`
4. Verify setup function: `grep 'def setup_jobs' backend/pipeline/scheduler.py`
5. Test import: `python -c "from backend.pipeline.scheduler import scheduler, setup_jobs; print('OK')"`
  </verify>
  <done>
Scheduler module exists with AsyncIOScheduler, job wrappers with execution logging, setup_jobs() function, and get_job_health() for monitoring.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate scheduler with FastAPI lifespan</name>
  <files>backend/main.py</files>
  <action>
Update backend/main.py to start/stop scheduler with FastAPI lifespan:

**Modify existing lifespan context manager:**

```python
from contextlib import asynccontextmanager
from backend.pipeline.scheduler import scheduler, setup_jobs, get_job_health
import structlog

logger = structlog.get_logger()

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("application_startup")

    # Database initialization (from Phase 1, assumed exists)
    # ...

    # Scheduler startup
    logger.info("scheduler_starting")
    setup_jobs()  # Register jobs
    scheduler.start()  # Start scheduler

    # Verify jobs registered
    jobs = scheduler.get_jobs()
    if not jobs:
        logger.warning("no_scheduler_jobs_registered")
    else:
        logger.info("scheduler_started", job_count=len(jobs), jobs=[j.id for j in jobs])

    yield

    # Shutdown
    logger.info("application_shutdown")

    # Scheduler shutdown
    logger.info("scheduler_stopping")
    scheduler.shutdown(wait=True)  # Wait for running jobs to complete
    logger.info("scheduler_stopped")

app = FastAPI(lifespan=lifespan)
```

**Integration requirements:**
- Call setup_jobs() BEFORE scheduler.start()
- Use scheduler.shutdown(wait=True) to gracefully finish running jobs
- Log job count and job IDs on startup for verification
- Do NOT call scheduler.start() multiple times (idempotent check if needed)

**Existing main.py structure (assumed from Phase 1):**
- FastAPI app instance exists
- Health endpoint router included
- CORS middleware configured (if applicable)

**DO NOT:**
- Remove existing lifespan logic (database init, etc.)
- Start scheduler outside lifespan context
- Call blocking scheduler.start() without async context
  </action>
  <verify>
1. Check lifespan modified: `grep -A 10 '@asynccontextmanager' backend/main.py | grep 'scheduler.start'`
2. Verify scheduler import: `grep 'from backend.pipeline.scheduler import' backend/main.py`
3. Test app startup:
   ```bash
   docker-compose up -d backend
   docker-compose logs backend | grep 'scheduler_started'
   ```
4. Verify jobs running:
   ```bash
   docker-compose logs backend | grep -E 'poll_news_job_started|poll_stories_job_started'
   ```
  </verify>
  <done>
FastAPI lifespan starts scheduler on app startup, registers 2 jobs, logs startup confirmation, and gracefully shuts down scheduler on app shutdown.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
APScheduler integrated with FastAPI, running news job every 15 minutes and stories job every 60 minutes, with execution logging and health monitoring.
  </what-built>
  <how-to-verify>
1. **Verify scheduler started:**
   ```bash
   docker-compose logs backend | grep 'scheduler_started'
   # Should show: scheduler_started with job_count=2
   ```

2. **Wait 1 minute and check for first news job execution:**
   ```bash
   docker-compose logs backend | grep 'poll_news_job_started'
   # Should show execution_id and timestamp
   ```

3. **Check job execution logs in database:**
   ```bash
   docker-compose exec postgres psql -U vibecheck -d vibecheck -c "SELECT job_name, status, duration_seconds FROM scheduler_execution_log ORDER BY started_at DESC LIMIT 5"
   # Should show recent poll_news executions
   ```

4. **Verify health endpoint (next task):**
   Wait until Task 4 completes, then test /health/scheduler endpoint.

5. **Check for errors:**
   ```bash
   docker-compose logs backend | grep -E 'error|failed|exception' | tail -20
   # Should NOT show scheduler-related errors (job failures OK if API key not set)
   ```

6. **Verify jobs continue after individual job failure:**
   - If ASKNEWS_API_KEY not set, jobs will fail but scheduler should continue
   - Check logs show next scheduled execution attempt

**Expected behavior:**
- News job attempts to run every 15 minutes (may fail if ASKNEWS_API_KEY not configured yet)
- Stories job attempts to run every 60 minutes
- Scheduler doesn't crash even if jobs fail
- Execution logs captured in database
  </how-to-verify>
  <resume-signal>Type "approved" when scheduler is running and jobs are executing on schedule, or describe issues found</resume-signal>
</task>

<task type="auto">
  <name>Task 4: Add scheduler health check endpoint</name>
  <files>backend/api/routes/health.py</files>
  <action>
Extend backend/api/routes/health.py to add scheduler health check endpoint:

**Add new endpoint:**

```python
from backend.pipeline.scheduler import get_job_health
from fastapi.responses import JSONResponse

@router.get("/health/scheduler")
async def scheduler_health():
    """
    Health check for scheduler jobs.

    Returns 200 if all jobs healthy, 503 if any job unhealthy or not started.

    Response:
    {
      "status": "healthy" | "unhealthy",
      "jobs": {
        "poll_news": {
          "status": "healthy" | "unhealthy" | "not_started",
          "last_run": "2026-02-05T12:00:00Z" | null,
          "age_seconds": 450
        },
        "poll_stories": { ... }
      }
    }
    """
    health = await get_job_health()

    status_code = 200 if health['status'] == 'healthy' else 503

    return JSONResponse(health, status_code=status_code)
```

**Health logic (implemented in scheduler.py get_job_health()):**
- Healthy: Last run within 2x expected interval
- Unhealthy: Last run exceeded 2x interval (e.g., news job hasn't run in 30+ min)
- Not started: Job never executed

**Existing health.py endpoints (assumed from Phase 1):**
- GET /health - Basic health check
- May include database connectivity check

**DO NOT:**
- Remove existing health endpoints
- Make this endpoint blocking (use async def)
- Query database directly (use get_job_health() from scheduler.py)
  </action>
  <verify>
1. Check endpoint exists: `grep 'GET.*health/scheduler' backend/api/routes/health.py` (via comment or route)
2. Test endpoint (after scheduler running for 1+ minute):
   ```bash
   curl http://localhost:8000/health/scheduler | jq
   # Should return 200 with status: "healthy" or "not_started"
   ```
3. Verify unhealthy detection (stop scheduler, wait 35 min, check again):
   Should return 503 with status: "unhealthy"
  </verify>
  <done>
Health endpoint /health/scheduler returns scheduler job status with 200 (healthy) or 503 (unhealthy) status codes.
  </done>
</task>

</tasks>

<verification>
Run these checks after task completion:

1. **Database migration applied:**
   ```bash
   docker-compose exec postgres psql -U vibecheck -d vibecheck -c "\d scheduler_execution_log"
   ```

2. **Scheduler running:**
   ```bash
   docker-compose logs backend | grep 'scheduler_started'
   ```

3. **Jobs executing:**
   ```bash
   docker-compose logs backend | grep -E 'poll_news_job_started|poll_stories_job_started' | tail -5
   ```

4. **Execution logs in database:**
   ```bash
   docker-compose exec postgres psql -U vibecheck -d vibecheck -c "SELECT job_name, status, started_at FROM scheduler_execution_log ORDER BY started_at DESC LIMIT 10"
   ```

5. **Health endpoint responds:**
   ```bash
   curl http://localhost:8000/health/scheduler | jq
   ```

6. **Jobs continue after failure:**
   - Unset ASKNEWS_API_KEY temporarily
   - Wait for job to fail
   - Check logs show next scheduled execution still occurs
</verification>

<success_criteria>
- SchedulerExecutionLog model stores execution history with status and metadata
- APScheduler starts with FastAPI lifespan and registers 2 jobs
- News job runs every 15 minutes, stories job runs every 60 minutes
- Job wrappers log execution start/completion with execution_id
- Health endpoint /health/scheduler reports job status and last run time
- Scheduler continues running even if individual jobs fail
- All logging uses structlog with execution_id and job names
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-pipeline/02-04-SUMMARY.md`
</output>

---
phase: 02-data-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["01"]
files_modified:
  - backend/pipeline/jobs/__init__.py
  - backend/pipeline/jobs/news_job.py
  - backend/pipeline/services/deduplication_service.py
  - backend/pipeline/services/storage_service.py
  - backend/db/models.py
autonomous: true

must_haves:
  truths:
    - "News job fetches articles from AskNews /news endpoint for all 10 entities"
    - "Articles deduplicate via external_id (primary) and URL hash (secondary)"
    - "Entity names normalize to canonical form before storage"
    - "Job continues processing remaining entities even if one entity fails"
    - "Retry logic with exponential backoff handles transient API errors"
  artifacts:
    - path: "backend/pipeline/jobs/news_job.py"
      provides: "Async news polling job with retry logic"
      min_lines: 100
      contains: "poll_news_job.*@retry.*fetch_from_asknews_with_retry"
    - path: "backend/pipeline/services/deduplication_service.py"
      provides: "URL hash deduplication checks"
      contains: "compute_url_hash.*check_article_exists"
    - path: "backend/pipeline/services/storage_service.py"
      provides: "Batch article insertion with normalization"
      contains: "batch_insert_articles.*normalize_entity_name"
    - path: "backend/db/models.py"
      provides: "Article model with url_hash field"
      contains: "url_hash.*Column.*String"
  key_links:
    - from: "backend/pipeline/jobs/news_job.py"
      to: "backend/pipeline/clients/asknews_client.py"
      via: "import AskNewsClient"
      pattern: "from backend\\.pipeline\\.clients\\.asknews_client import AskNewsClient"
    - from: "backend/pipeline/jobs/news_job.py"
      to: "tenacity retry decorator"
      via: "import retry"
      pattern: "from tenacity import retry"
    - from: "backend/pipeline/services/storage_service.py"
      to: "backend/pipeline/services/entity_service.py"
      via: "import normalize_entity_name"
      pattern: "from backend\\.pipeline\\.services\\.entity_service import normalize_entity_name"
---

<objective>
Implement news ingestion job with deduplication and retry logic.

Purpose: Build the core news polling job that fetches articles from AskNews every 15 minutes (scheduling in Plan 04), handles API failures gracefully, and prevents duplicate articles from accumulating.
Output: Working news job that can be called manually or scheduled, with proven deduplication and entity normalization.
</objective>

<execution_context>
@/Users/daniswhoiam/.claude/get-shit-done/workflows/execute-plan.md
@/Users/daniswhoiam/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-pipeline/02-RESEARCH.md

# From Plan 01
AskNewsClient with fetch_news() method in backend/pipeline/clients/asknews_client.py
normalize_entity_name() in backend/pipeline/services/entity_service.py
ENTITY_VARIATIONS dict in backend/utils/constants.py

# From Phase 1
Article model in backend/db/models.py (will add url_hash field)
AsyncSession in backend/db/session.py
CURATED_ENTITIES list in backend/utils/constants.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add url_hash field to Article model</name>
  <files>backend/db/models.py</files>
  <action>
Update the Article model in backend/db/models.py to add url_hash field for deduplication:

**Add to Article class:**

```python
url_hash: Mapped[str] = mapped_column(String(64), nullable=False, index=True)
```

**Field requirements:**
- SHA256 hash produces 64-character hex string
- Index on url_hash for fast deduplication queries
- NOT unique (multiple articles can theoretically have same hash)
- NOT nullable (compute hash from url field)

**Index strategy:**
- external_id has UNIQUE constraint (from Phase 1, assumed)
- url_hash has INDEX for fast lookups but not UNIQUE
- Combined strategy: check external_id first (unique), then url_hash (indexed)

**Migration note:**
This change requires Alembic migration. After modifying model, run:
```bash
alembic revision --autogenerate -m "add url_hash to articles"
alembic upgrade head
```

DO NOT run migration in this task - just modify the model. Migration will be run during verification.
  </action>
  <verify>
1. Check url_hash field exists: `grep 'url_hash.*Mapped' backend/db/models.py`
2. Generate migration: `docker-compose exec backend alembic revision --autogenerate -m "add url_hash to articles"`
3. Apply migration: `docker-compose exec backend alembic upgrade head`
4. Verify schema: `docker-compose exec postgres psql -U vibecheck -d vibecheck -c "\d articles"`
  </verify>
  <done>
Article model has url_hash field (String 64, indexed). Migration generated and applied successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create deduplication service</name>
  <files>backend/pipeline/services/deduplication_service.py</files>
  <action>
Create backend/pipeline/services/deduplication_service.py with URL-based deduplication:

**Implement three functions:**

1. **compute_url_hash(url: str) -> str:**
   - Import hashlib (standard library)
   - Generate SHA256 hash: `hashlib.sha256(url.encode()).hexdigest()`
   - Return 64-character hex string
   - DO NOT strip query parameters (dedup exact URLs only)

2. **async check_article_exists(external_id: str, url: str, db_session) -> bool:**
   - Primary check: Query Article table WHERE external_id = %s LIMIT 1
   - If found, log "article_duplicate_by_external_id" and return True
   - Secondary check: Compute url_hash, query Article table WHERE url_hash = %s LIMIT 1
   - If found, log "article_duplicate_by_url" and return True
   - If neither found, return False
   - Use structlog for debug logging

3. **async batch_check_duplicates(articles: list[dict], db_session) -> tuple[list[dict], int]:**
   - For each article, call check_article_exists()
   - Filter out duplicates
   - Return (to_insert: list[dict], duplicates_skipped: int)
   - Log summary: "deduplication_complete" with attempted count and duplicates_skipped count

**Database queries:**
- Use SQLAlchemy select() with filter() for type safety
- Execute via db_session.execute()
- Example: `result = await db_session.execute(select(Article).filter(Article.external_id == external_id).limit(1))`

**Error handling:**
- Propagate database exceptions (do NOT catch and return False)
- Log errors with structlog.error() and exc_info=True

**DO NOT:**
- Use UNIQUE constraints (blocks batch inserts)
- Pre-fetch all existing external_ids into memory (doesn't scale)
- Implement caching (premature optimization)
  </action>
  <verify>
1. Check file exists: `ls backend/pipeline/services/deduplication_service.py`
2. Verify functions: `grep -E 'def compute_url_hash|async def check_article_exists|async def batch_check_duplicates' backend/pipeline/services/deduplication_service.py`
3. Test URL hash: `python -c "from backend.pipeline.services.deduplication_service import compute_url_hash; print(len(compute_url_hash('https://example.com')))"` (should print 64)
  </verify>
  <done>
Deduplication service exists with compute_url_hash(), check_article_exists(), and batch_check_duplicates() functions. URL hashing works correctly.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create storage service for batch article insertion</name>
  <files>backend/pipeline/services/storage_service.py</files>
  <action>
Create backend/pipeline/services/storage_service.py for normalized article storage:

**Implement:**

**async batch_insert_articles(articles: list[dict], db_session) -> int:**

1. **Input validation:**
   - Return 0 if articles list is empty
   - Each article dict must have keys: external_id, url, entity_name, title, sentiment, source_url, published_at

2. **Deduplication:**
   - Call batch_check_duplicates(articles, db_session) from deduplication_service
   - Get filtered list (to_insert) and duplicates_skipped count

3. **Entity normalization:**
   - For each article in to_insert:
     - Call normalize_entity_name(article['entity_name'])
     - If canonical name is None, skip article (not in curated list)
     - Store both canonical name (entity_name) and original (extracted_entity_name)
   - Track skipped_non_curated count

4. **Compute URL hashes:**
   - For each article, compute url_hash = compute_url_hash(article['url'])
   - Add url_hash to article dict

5. **Batch insert with SQLAlchemy:**
   - Use db_session.execute(insert(Article).values(...)) with executemany for batch
   - Alternatively, create Article instances and db_session.add_all()
   - Use ON CONFLICT DO NOTHING for external_id (safety net for race conditions)
   - Commit transaction

6. **Logging:**
   - Log "batch_insert_completed" with counts: attempted, duplicates_skipped, non_curated_skipped, inserted
   - Return inserted count

**Error handling:**
- Wrap insert in try/except
- On database error, log and raise (do NOT swallow errors)
- Use async context manager for transaction if needed

**DO NOT:**
- Insert articles one-by-one (batch insert is 10x faster)
- Create Entity records (assume entities exist from Phase 1)
- Validate sentiment range (trust AskNews data)
  </action>
  <verify>
1. Check file exists: `ls backend/pipeline/services/storage_service.py`
2. Verify function: `grep 'async def batch_insert_articles' backend/pipeline/services/storage_service.py`
3. Verify imports: `grep -E 'from.*entity_service import normalize_entity_name|from.*deduplication_service import' backend/pipeline/services/storage_service.py`
  </verify>
  <done>
Storage service implements batch_insert_articles() with entity normalization, deduplication filtering, URL hashing, and batch database insertion.
  </done>
</task>

<task type="auto">
  <name>Task 4: Implement news polling job with retry logic</name>
  <files>backend/pipeline/jobs/news_job.py, backend/pipeline/jobs/__init__.py</files>
  <action>
Create backend/pipeline/jobs/news_job.py implementing the scheduled news polling job:

**Imports:**
- tenacity: retry, stop_after_attempt, wait_exponential, retry_if_exception_type
- datetime, UTC
- uuid
- structlog
- AskNewsClient from backend.pipeline.clients.asknews_client
- batch_insert_articles from backend.pipeline.services.storage_service
- CURATED_ENTITIES from backend.utils.constants (assume list of canonical entity names)
- AsyncSession from backend.db.session

**Implementation:**

1. **fetch_from_asknews_with_retry(entity_name: str, client: AskNewsClient) -> list[dict]:**
   - Decorate with @retry:
     - stop=stop_after_attempt(3)
     - wait=wait_exponential(multiplier=1, min=1, max=16)
     - retry=retry_if_exception_type((TimeoutError, ConnectionError))
     - reraise=True
   - Call await client.fetch_news(entity_name, limit=10)
   - Return articles list
   - On failure after 3 retries, exception propagates to caller

2. **async poll_news_job(db_session: AsyncSession) -> dict:**
   - Generate execution_id = str(uuid.uuid4())
   - Record start_time = datetime.now(UTC)
   - Log "poll_news_job_started" with execution_id and timestamp

   - Initialize counters: articles_stored = 0, entities_processed = 0, entities_failed = 0

   - Create AskNewsClient instance with api_key from environment

   - **For each entity in CURATED_ENTITIES:**
     - Try:
       - Call fetch_from_asknews_with_retry(entity, client)
       - Call batch_insert_articles(articles, db_session)
       - Increment articles_stored by returned count
       - Increment entities_processed
       - Log "entity_processed" with execution_id, entity, articles_found, articles_stored
     - Except Exception as e:
       - Increment entities_failed
       - Log error with "entity_processing_failed", execution_id, entity, error
       - **CONTINUE** to next entity (do NOT fail entire job)

   - Calculate duration = (datetime.now(UTC) - start_time).total_seconds()
   - Log "poll_news_job_completed" with execution_id, articles_stored, entities_processed, entities_failed, duration, status="success"
   - Return dict with execution stats

   - **If job-level exception (should be rare):**
     - Log "poll_news_job_failed" with execution_id, error, duration
     - Raise exception

**CURATED_ENTITIES:**
Assume backend/utils/constants.py has CURATED_ENTITIES list (if not, extract from ENTITY_VARIATIONS keys and map to canonical names).

**Job signature:**
Accept db_session as parameter (scheduler will provide it in Plan 04).

**DO NOT:**
- Implement scheduler here (Plan 04 handles scheduling)
- Create database session (caller provides it)
- Stop processing on first entity failure (continue with remaining entities)

**Create backend/pipeline/jobs/__init__.py:**
Empty file (package marker).
  </action>
  <verify>
1. Check file exists: `ls backend/pipeline/jobs/news_job.py`
2. Verify retry decorator: `grep '@retry' backend/pipeline/jobs/news_job.py`
3. Verify job function: `grep 'async def poll_news_job' backend/pipeline/jobs/news_job.py`
4. Test import: `python -c "from backend.pipeline.jobs.news_job import poll_news_job; print('OK')"`
5. Manual test (requires ASKNEWS_API_KEY set):
   ```python
   from backend.pipeline.jobs.news_job import poll_news_job
   from backend.db.session import get_session
   async with get_session() as session:
       result = await poll_news_job(session)
       print(result)
   ```
  </verify>
  <done>
News polling job exists with tenacity retry logic (3 attempts, exponential backoff), per-entity error handling, batch article insertion, and comprehensive structured logging.
  </done>
</task>

</tasks>

<verification>
Run these checks after task completion:

1. **Database migration applied:**
   ```bash
   docker-compose exec postgres psql -U vibecheck -d vibecheck -c "SELECT column_name FROM information_schema.columns WHERE table_name='articles' AND column_name='url_hash'"
   ```

2. **News job can be imported:**
   ```bash
   docker-compose exec backend python -c "from backend.pipeline.jobs.news_job import poll_news_job; print('OK')"
   ```

3. **Deduplication functions exist:**
   ```bash
   docker-compose exec backend python -c "from backend.pipeline.services.deduplication_service import compute_url_hash; print(len(compute_url_hash('https://example.com')))"
   ```

4. **Manual job execution (requires ASKNEWS_API_KEY):**
   ```bash
   # Set ASKNEWS_API_KEY in backend/.env first
   docker-compose exec backend python -c "
   import asyncio
   from backend.pipeline.jobs.news_job import poll_news_job
   from backend.db.session import get_async_session

   async def test():
       async with get_async_session() as session:
           result = await poll_news_job(session)
           print(f'Articles stored: {result}')

   asyncio.run(test())
   "
   ```
</verification>

<success_criteria>
- Article model has url_hash field (String 64, indexed, not unique)
- Deduplication service checks external_id (primary) then url_hash (secondary)
- Storage service normalizes entities, filters duplicates, and batch inserts
- News job fetches from AskNews with 3-attempt retry and exponential backoff
- Job continues processing entities even if one fails
- All logging uses structlog with execution_id, entity names, and counts
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-pipeline/02-02-SUMMARY.md`
</output>

---
phase: 02-data-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["01"]
files_modified:
  - backend/pipeline/jobs/stories_job.py
  - backend/pipeline/services/sentiment_service.py
  - backend/db/models.py
autonomous: true

must_haves:
  truths:
    - "Story job fetches clusters from AskNews /stories endpoint for all 10 entities"
    - "Sentiment time-series data stores hourly aggregates per entity"
    - "Reddit sentiment data extracts from story clusters with thread metadata"
    - "Job handles missing sentiment data gracefully (some stories may lack Reddit data)"
  artifacts:
    - path: "backend/pipeline/jobs/stories_job.py"
      provides: "Async story polling job with retry logic"
      min_lines: 100
      contains: "poll_stories_job.*@retry.*fetch_from_asknews_with_retry"
    - path: "backend/pipeline/services/sentiment_service.py"
      provides: "Sentiment time-series storage and aggregation"
      contains: "store_sentiment_timeseries.*aggregate_story_sentiment"
    - path: "backend/db/models.py"
      provides: "SentimentTimeseries model with reddit_sentiment field"
      contains: "reddit_sentiment.*Column"
  key_links:
    - from: "backend/pipeline/jobs/stories_job.py"
      to: "backend/pipeline/clients/asknews_client.py"
      via: "import AskNewsClient"
      pattern: "from backend\\.pipeline\\.clients\\.asknews_client import AskNewsClient"
    - from: "backend/pipeline/services/sentiment_service.py"
      to: "backend/db/models.py"
      via: "import SentimentTimeseries"
      pattern: "from backend\\.db\\.models import SentimentTimeseries"
---

<objective>
Implement story ingestion job with sentiment time-series and Reddit data extraction.

Purpose: Build the story polling job that fetches narrative clusters from AskNews every 60 minutes (scheduling in Plan 04), extracting both overall sentiment trends and Reddit-specific sentiment for community opinion tracking.
Output: Working story job that stores sentiment time-series aggregates and Reddit thread data for frontend consumption.
</objective>

<execution_context>
@/Users/daniswhoiam/.claude/get-shit-done/workflows/execute-plan.md
@/Users/daniswhoiam/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-data-pipeline/02-RESEARCH.md

# From Plan 01
AskNewsClient with fetch_stories() method in backend/pipeline/clients/asknews_client.py
normalize_entity_name() in backend/pipeline/services/entity_service.py
ENTITY_VARIATIONS dict in backend/utils/constants.py

# From Phase 1
SentimentTimeseries model in backend/db/models.py (will add reddit_sentiment field)
AsyncSession in backend/db/session.py
CURATED_ENTITIES list in backend/utils/constants.py

# NOTE: AskNews /stories response structure needs validation
Research indicates stories include sentiment time-series and Reddit data, but exact schema is unclear.
This plan logs full responses for first story to document actual structure.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add reddit_sentiment field to SentimentTimeseries model</name>
  <files>backend/db/models.py</files>
  <action>
Update the SentimentTimeseries model in backend/db/models.py to support Reddit-specific sentiment:

**Add to SentimentTimeseries class:**

```python
# Separate Reddit sentiment from overall news sentiment
reddit_sentiment: Mapped[float | None] = mapped_column(Float, nullable=True)
reddit_thread_count: Mapped[int | None] = mapped_column(Integer, nullable=True, default=0)
```

**Field requirements:**
- reddit_sentiment: Nullable float for Reddit community sentiment (-1.0 to 1.0)
- reddit_thread_count: Number of Reddit threads contributing to sentiment aggregate
- Both nullable because not all stories have Reddit data

**Existing fields (from Phase 1, assumed):**
- entity_id (foreign key to Entity)
- timestamp (datetime with timezone)
- sentiment (overall story sentiment)
- source (e.g., "asknews_story")

**Migration:**
After modifying model, generate and apply migration:
```bash
alembic revision --autogenerate -m "add reddit_sentiment to sentiment_timeseries"
alembic upgrade head
```

DO NOT run migration in this task - just modify the model. Migration will be run during verification.
  </action>
  <verify>
1. Check fields exist: `grep -E 'reddit_sentiment|reddit_thread_count' backend/db/models.py`
2. Generate migration: `docker-compose exec backend alembic revision --autogenerate -m "add reddit_sentiment to sentiment_timeseries"`
3. Apply migration: `docker-compose exec backend alembic upgrade head`
4. Verify schema: `docker-compose exec postgres psql -U vibecheck -d vibecheck -c "\d sentiment_timeseries"`
  </verify>
  <done>
SentimentTimeseries model has reddit_sentiment (Float nullable) and reddit_thread_count (Integer nullable) fields. Migration applied successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create sentiment time-series storage service</name>
  <files>backend/pipeline/services/sentiment_service.py</files>
  <action>
Create backend/pipeline/services/sentiment_service.py for sentiment aggregation and storage:

**Implement two functions:**

1. **async store_sentiment_timeseries(entity_id: int, timestamp: datetime, sentiment: float, reddit_sentiment: float | None, reddit_thread_count: int | None, source: str, db_session) -> bool:**
   - Create SentimentTimeseries instance with provided data
   - Use ON CONFLICT (entity_id, timestamp, source) DO UPDATE to handle duplicates
   - Update sentiment values if newer data available
   - Commit to database
   - Log "sentiment_stored" with entity_id, timestamp, sentiment, reddit_sentiment
   - Return True if inserted/updated, False if skipped
   - Handle database errors with logging and exception propagation

2. **extract_story_sentiment(story_data: dict) -> dict:**
   - Parse AskNews /stories response to extract sentiment data
   - Expected story_data keys (validate against real API response in Plan 01):
     - story_id: str
     - headline: str
     - sentiment_timeseries: list[dict] with timestamp + sentiment
     - reddit_threads: list[dict] (optional) with sentiment per thread
   - Aggregate sentiment time-series into hourly buckets
   - Calculate average Reddit sentiment across threads
   - Return dict with:
     ```python
     {
       'story_id': str,
       'headline': str,
       'timeseries': [
         {'timestamp': datetime, 'sentiment': float},
         ...
       ],
       'reddit_sentiment': float | None,
       'reddit_thread_count': int
     }
     ```
   - Log warning if sentiment_timeseries is empty or malformed
   - Return empty dict if story_data is invalid

**Sentinel time-series aggregation:**
- Group sentiment by hour (round timestamp to hour)
- Average sentiment values within each hour
- Store one record per entity per hour

**Reddit sentiment aggregation:**
- Average sentiment across all reddit_threads
- Count total threads
- Return None if no Reddit data

**Error handling:**
- Validate story_data structure before processing
- Handle missing fields gracefully (return None/0 for optional data)
- Log parsing errors with full story_data for debugging

**DO NOT:**
- Implement sentiment forecasting (out of scope for Phase 2)
- Cache aggregates in memory (write directly to DB)
- Filter sentiment outliers (trust AskNews data)
  </action>
  <verify>
1. Check file exists: `ls backend/pipeline/services/sentiment_service.py`
2. Verify functions: `grep -E 'async def store_sentiment_timeseries|def extract_story_sentiment' backend/pipeline/services/sentiment_service.py`
3. Test story extraction with mock data:
   ```python
   from backend.pipeline.services.sentiment_service import extract_story_sentiment
   test_story = {
       'story_id': 'test-123',
       'headline': 'Test Story',
       'sentiment_timeseries': [
           {'timestamp': '2026-02-05T12:00:00Z', 'sentiment': 0.5},
           {'timestamp': '2026-02-05T12:30:00Z', 'sentiment': 0.7}
       ],
       'reddit_threads': [
           {'sentiment': 0.6},
           {'sentiment': 0.4}
       ]
   }
   result = extract_story_sentiment(test_story)
   assert result['reddit_sentiment'] == 0.5
   ```
  </verify>
  <done>
Sentiment service implements store_sentiment_timeseries() for database writes and extract_story_sentiment() for parsing AskNews /stories responses with Reddit data aggregation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement story polling job with Reddit sentiment extraction</name>
  <files>backend/pipeline/jobs/stories_job.py</files>
  <action>
Create backend/pipeline/jobs/stories_job.py implementing the scheduled story polling job:

**Imports:**
- tenacity: retry, stop_after_attempt, wait_exponential, retry_if_exception_type
- datetime, UTC
- uuid
- structlog
- AskNewsClient from backend.pipeline.clients.asknews_client
- extract_story_sentiment, store_sentiment_timeseries from backend.pipeline.services.sentiment_service
- normalize_entity_name, get_entity_id_by_name from backend.pipeline.services.entity_service
- CURATED_ENTITIES from backend.utils.constants
- AsyncSession from backend.db.session

**Implementation:**

1. **fetch_stories_from_asknews_with_retry(entity_name: str, client: AskNewsClient) -> list[dict]:**
   - Decorate with @retry:
     - stop=stop_after_attempt(3)
     - wait=wait_exponential(multiplier=1, min=1, max=16)
     - retry=retry_if_exception_type((TimeoutError, ConnectionError))
     - reraise=True
   - Call await client.fetch_stories(entity_name, limit=10)
   - Return stories list
   - On failure after 3 retries, exception propagates

2. **async poll_stories_job(db_session: AsyncSession) -> dict:**
   - Generate execution_id = str(uuid.uuid4())
   - Record start_time = datetime.now(UTC)
   - Log "poll_stories_job_started" with execution_id and timestamp

   - Initialize counters: sentiment_records_stored = 0, entities_processed = 0, entities_failed = 0, stories_without_reddit = 0

   - Create AskNewsClient instance with api_key from environment

   - **For each entity in CURATED_ENTITIES:**
     - Try:
       - Get entity_id = get_entity_id_by_name(entity, db_session)
       - If entity_id is None, log warning and skip (entity not in database)
       - Call fetch_stories_from_asknews_with_retry(entity, client)
       - **For each story in stories:**
         - Extract sentiment data: extracted = extract_story_sentiment(story)
         - If extracted is empty, log warning and skip
         - **For each timeseries point in extracted['timeseries']:**
           - Call store_sentiment_timeseries(
               entity_id,
               timeseries['timestamp'],
               timeseries['sentiment'],
               extracted['reddit_sentiment'],
               extracted['reddit_thread_count'],
               source='asknews_story',
               db_session
             )
           - Increment sentiment_records_stored if True
         - If reddit_sentiment is None, increment stories_without_reddit
       - Increment entities_processed
       - Log "entity_stories_processed" with execution_id, entity, stories_found, sentiment_records_stored, stories_without_reddit
     - Except Exception as e:
       - Increment entities_failed
       - Log error with "entity_story_processing_failed", execution_id, entity, error
       - **CONTINUE** to next entity (do NOT fail entire job)

   - Calculate duration = (datetime.now(UTC) - start_time).total_seconds()
   - Log "poll_stories_job_completed" with execution_id, sentiment_records_stored, entities_processed, entities_failed, stories_without_reddit, duration, status="success"
   - Return dict with execution stats

   - **If job-level exception:**
     - Log "poll_stories_job_failed" with execution_id, error, duration
     - Raise exception

**Special logging for first run:**
- On first story for first entity, log full story_data dict at INFO level
- Add comment: "# Log full story structure for validation (first run only)"
- This helps validate AskNews /stories response schema assumptions

**Job signature:**
Accept db_session as parameter (scheduler will provide it in Plan 04).

**DO NOT:**
- Implement scheduler here (Plan 04 handles scheduling)
- Create database session (caller provides it)
- Stop processing on first entity failure (continue with remaining entities)
- Fail job if some stories lack Reddit data (log count and continue)
  </action>
  <verify>
1. Check file exists: `ls backend/pipeline/jobs/stories_job.py`
2. Verify retry decorator: `grep '@retry' backend/pipeline/jobs/stories_job.py`
3. Verify job function: `grep 'async def poll_stories_job' backend/pipeline/jobs/stories_job.py`
4. Test import: `python -c "from backend.pipeline.jobs.stories_job import poll_stories_job; print('OK')"`
5. Manual test (requires ASKNEWS_API_KEY set):
   ```python
   from backend.pipeline.jobs.stories_job import poll_stories_job
   from backend.db.session import get_session
   async with get_session() as session:
       result = await poll_stories_job(session)
       print(result)
   ```
  </verify>
  <done>
Story polling job exists with tenacity retry logic, Reddit sentiment extraction, sentiment time-series storage, per-entity error handling, and comprehensive structured logging.
  </done>
</task>

</tasks>

<verification>
Run these checks after task completion:

1. **Database migration applied:**
   ```bash
   docker-compose exec postgres psql -U vibecheck -d vibecheck -c "SELECT column_name FROM information_schema.columns WHERE table_name='sentiment_timeseries' AND column_name IN ('reddit_sentiment', 'reddit_thread_count')"
   ```

2. **Story job can be imported:**
   ```bash
   docker-compose exec backend python -c "from backend.pipeline.jobs.stories_job import poll_stories_job; print('OK')"
   ```

3. **Sentiment service exists:**
   ```bash
   docker-compose exec backend python -c "from backend.pipeline.services.sentiment_service import extract_story_sentiment; print('OK')"
   ```

4. **Manual job execution (requires ASKNEWS_API_KEY):**
   ```bash
   # Set ASKNEWS_API_KEY in backend/.env first
   docker-compose exec backend python -c "
   import asyncio
   from backend.pipeline.jobs.stories_job import poll_stories_job
   from backend.db.session import get_async_session

   async def test():
       async with get_async_session() as session:
           result = await poll_stories_job(session)
           print(f'Sentiment records stored: {result}')

   asyncio.run(test())
   "
   ```

5. **Check first-run logging for story structure validation:**
   ```bash
   docker-compose logs backend | grep "story_data"
   ```
</verification>

<success_criteria>
- SentimentTimeseries model has reddit_sentiment and reddit_thread_count fields
- Sentiment service extracts time-series and Reddit data from story responses
- Story job fetches from AskNews with 3-attempt retry and exponential backoff
- Job stores hourly sentiment aggregates with separate Reddit sentiment
- Job continues processing entities even if one fails
- Job handles missing Reddit data gracefully (logs count, continues)
- All logging uses structlog with execution_id, entity names, and counts
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-pipeline/02-03-SUMMARY.md`
</output>
